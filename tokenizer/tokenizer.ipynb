{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ab380",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e0e1a",
   "metadata": {},
   "source": [
    "MC-MED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_NAME = ['event']\n",
    "COLS = {\n",
    "    \"labs\": ['Component_name'],\n",
    "    \"numerics\": ['Measure'],\n",
    "    \"orders\": ['Procedure_ID'],\n",
    "    \"rads\": ['Study']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9efc5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_fn(fn):\n",
    "    cols = COLS[fn]\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, f'{fn}.csv'), usecols=['CSN'] + cols)\n",
    "    df = df.rename(columns={c: COLS_NAME[i] for i, c in enumerate(cols)})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdeec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_from_data(FNAME, exclude_demo=False):\n",
    "    labs_df = read_csv_fn('labs')\n",
    "    vitals_df = read_csv_fn('numerics')\n",
    "    orders_df = read_csv_fn('orders')\n",
    "    df = pd.concat([labs_df, vitals_df, orders_df])\n",
    "\n",
    "    if exclude_demo:\n",
    "        vocab_list = list(set(df['event'].astype(str).unique()))\n",
    "    else:\n",
    "        vocab_list = list(set().union(*df['input'])) + list(df['age'].unique()) + list(df['sex'].unique()) + list(df['race'].unique())\n",
    "    vocab_list = [x for x in vocab_list if x is not None]\n",
    "    vocab_list = list(set(vocab_list))\n",
    "    \n",
    "    special_list = ['[UNK]', '[BOS]', '[EOS]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "    vocab_list = special_list + vocab_list\n",
    "\n",
    "    # Create a dictionary mapping each token to a unique index\n",
    "    vocab = {token: idx for idx, token in enumerate(vocab_list)}\n",
    "\n",
    "    # Create a WordLevel tokenizer with the custom vocab and an unknown token\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "\n",
    "    tokenizer.save(os.path.join(OUTPUT_DIR, f'{FNAME}_tokenizer.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_from_df(FNAME, exclude_demo=False):\n",
    "    df = pd.read_parquet('data/decomp_data.parquet')\n",
    "\n",
    "    vocab_list = list(set(df['eventval'].astype(str).unique()))\n",
    "    vocab_list = [x for x in vocab_list if x is not None]\n",
    "    vocab_list = list(set(vocab_list))\n",
    "    \n",
    "    special_list = ['[UNK]', '[BOS]', '[EOS]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "    vocab_list = special_list + vocab_list\n",
    "\n",
    "    # Create a dictionary mapping each token to a unique index\n",
    "    vocab = {token: idx for idx, token in enumerate(vocab_list)}\n",
    "\n",
    "    # Create a WordLevel tokenizer with the custom vocab and an unknown token\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "\n",
    "    tokenizer.save(os.path.join(OUTPUT_DIR, f'{FNAME}_tokenizer.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38efa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_from_df(FNAME, exclude_demo=False):\n",
    "    vocab_list = []\n",
    "\n",
    "    for i in range(0, 34):\n",
    "        df = pd.read_parquet(f'data/output_{i}.parquet')\n",
    "        df = df.dropna(subset=['eventval'])\n",
    "        vocab_list += list(set(df['eventval'].astype(str).unique()))\n",
    "        vocab_list = list(set(vocab_list))\n",
    "    \n",
    "    special_list = ['[UNK]', '[BOS]', '[EOS]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "    vocab_list = special_list + vocab_list\n",
    "\n",
    "    # Create a dictionary mapping each token to a unique index\n",
    "    vocab = {token: idx for idx, token in enumerate(vocab_list)}\n",
    "\n",
    "    # Create a WordLevel tokenizer with the custom vocab and an unknown token\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "\n",
    "    tokenizer.save(os.path.join(OUTPUT_DIR, f'{FNAME}_tokenizer.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_from_df(\"decomp\", exclude_demo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64297147",
   "metadata": {},
   "source": [
    "EHRSHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917922e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40612fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eb8cd79",
   "metadata": {},
   "source": [
    "MIMIC: ED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebbc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = pd.read_csv('data/transfer/demo.csv')\n",
    "med_df = pd.read_csv('data/transfer/med.csv')\n",
    "numerics_df = pd.read_csv('data/transfer/numerics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_vocab = list(demo_df['race_str'].unique()) + list(demo_df['gender_str'].unique()) + list(demo_df['age_str'].unique())\n",
    "med_vocab = list(med_df['eventval'].unique())\n",
    "num_vocab = list(numerics_df['eventval'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32247f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = demo_vocab + med_vocab + num_vocab\n",
    "vocab_list = [v for v in vocab_list if not v is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367fc82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_list = ['[UNK]', '[BOS]', '[EOS]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "vocab_list = special_list + vocab_list\n",
    "# Create a dictionary mapping each token to a unique index\n",
    "vocab = {token: idx for idx, token in enumerate(vocab_list)}\n",
    "\n",
    "# Create a WordLevel tokenizer with the custom vocab and an unknown token\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.save(os.path.join(OUTPUT_DIR, f'ed_eventval_tokenizer.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2a0eb",
   "metadata": {},
   "source": [
    "MIMIC: HOSP + ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18380134",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = pd.read_parquet('data/mortality/demographics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_vocab = list(demo_df['gender_str'].unique()) + list(demo_df['race_str'].unique()) + list(demo_df['ethnicity_str'].unique()) + list(demo_df['age_str'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e18b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventvals = []\n",
    "for i in range(0, 37):\n",
    "    df = pd.read_parquet(f'data/mortality/{i}_final.parquet', columns=['eventval'])\n",
    "    eventvals += list(df['eventval'].unique())\n",
    "    eventvals = list(set(eventvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = demo_vocab + eventvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_list = ['[UNK]', '[BOS]', '[EOS]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "vocab_list = special_list + vocab_list\n",
    "\n",
    "# Create a dictionary mapping each token to a unique index\n",
    "vocab = {token: idx for idx, token in enumerate(vocab_list)}\n",
    "\n",
    "# Create a WordLevel tokenizer with the custom vocab and an unknown token\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.save(os.path.join(OUTPUT_DIR, f'eventval_tokenizer.json'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
