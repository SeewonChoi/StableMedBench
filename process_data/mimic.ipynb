{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9477d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8773d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/mimiciv/3.1'\n",
    "TASK_DIR = '/data'\n",
    "LABEL_COLS = ['Acute and unspecified renal failure', 'Cardiac dysrhythmias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94578b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADM = ['subject_id', 'hadm_id', 'edregtime', 'edouttime', 'admittime', 'dischtime', 'deathtime',]\n",
    "PAT = ['subject_id', 'anchor_year', 'dod']\n",
    "PROC_ICD = ['subject_id', 'hadm_id', 'seq_num', 'chartdate', 'icd_code', 'icd_version']\n",
    "PHARM = ['subject_id', 'hadm_id', 'poe_id', 'starttime', 'stoptime', 'medication']\n",
    "EMAR = ['subject_id', 'hadm_id', 'emar_id', 'emar_seq', 'poe_id', 'charttime', 'medication']\n",
    "LAB = ['labevent_id', 'subject_id', 'hadm_id', 'specimen_id', 'itemid',\n",
    "       'charttime',  'value', 'valuenum', 'ref_range_lower', 'ref_range_upper', 'flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d603178",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAY = ['subject_id', 'hadm_id', 'stay_id', 'intime', 'outtime']\n",
    "CHART = ['subject_id', 'hadm_id', 'stay_id', 'charttime', 'itemid', 'value', 'valuenum', 'warning']\n",
    "INPUT = ['subject_id', 'hadm_id', 'stay_id', 'starttime', 'endtime', 'itemid', 'amount',  \n",
    "         'rate', 'patientweight', 'totalamount', 'originalamount', 'originalrate']\n",
    "OUTPUT = ['subject_id', 'hadm_id', 'stay_id', 'charttime', 'itemid', 'value']\n",
    "PROC = ['subject_id', 'hadm_id', 'stay_id', 'starttime', 'endtime', \n",
    "            'itemid', 'value', 'patientweight', 'originalamount', 'originalrate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids():\n",
    "    label_df3 = pd.read_csv(os.path.join(TASK_DIR, 'in-hospital-mortality', 'test/listfile.csv'), usecols=['stay'])\n",
    "    label_df4 = pd.read_csv(os.path.join(TASK_DIR, 'in-hospital-mortality', 'train/listfile.csv'), usecols=['stay'])\n",
    "\n",
    "    label_ids = []\n",
    "    label_ids += list(pd.to_numeric(label_df3['stay'].apply(lambda x: x.split('_', 1)[0])).unique())\n",
    "    label_ids += list(pd.to_numeric(label_df4['stay'].apply(lambda x: x.split('_', 1)[0])).unique())\n",
    "    \n",
    "    label_ids = list(set(label_ids))\n",
    "    return label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv(folder, fname, cols, label_ids):\n",
    "    print(fname)\n",
    "    filtered_chunks = []\n",
    "\n",
    "    for chunk in pd.read_csv(os.path.join(DATA_DIR, folder, f'{fname}.csv'), usecols=cols, chunksize=CHUNK_SIZE):\n",
    "        filtered_chunk = chunk[chunk['subject_id'].isin(label_ids)]\n",
    "        filtered_chunks.append(filtered_chunk)\n",
    "\n",
    "    df = pd.concat(filtered_chunks, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ids()\n",
    "for i in range(7):\n",
    "    print(f\"{i}\")\n",
    "    label_ids = pd.read_parquet(f'data/raw_data/visits_{i}.parquet')['subject_id']\n",
    "\n",
    "    folders = ['meta']*7\n",
    "    fnames = ['emar', 'labevents',  'pharmacy', \n",
    "              'chartevents', 'inputevents', 'outputevents', 'procedureevents']\n",
    "    # 'admissions', 'patients','icustays',\n",
    "    cols = [EMAR, LAB, PHARM, CHART, INPUT, OUTPUT, PROC]\n",
    "\n",
    "    for folder, fname, col in zip(folders, fnames, cols):\n",
    "        df = get_csv(folder, fname, col, label_ids)\n",
    "        df.to_parquet(f\"data/raw_data/{fname}_{i}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4fd9ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import linregress, kurtosis, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a14936",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = 'data/'\n",
    "TASK = 'in-hospital-mortality'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23583cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES = os.listdir(os.path.join(DATA_DIR, 'train'))\n",
    "TEST_FILES = os.listdir(os.path.join(DATA_DIR, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d003639",
   "metadata": {},
   "source": [
    "TO PARQUET: EVENT, VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_parquet(split, files):\n",
    "    for files_i in files:\n",
    "        if files_i[-4:] != '.csv' or files_i == 'listfile.csv': continue\n",
    "        fname = files_i[:-4]\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, f'{split}/{files_i}'))\n",
    "        \n",
    "        df_final = None\n",
    "        for col_i in df.columns:\n",
    "            if col_i == 'Hours': continue\n",
    "            if not 'Hours' in df.columns: \n",
    "                print(df.columns)\n",
    "                continue\n",
    "            df_i = df[[col_i, 'Hours']].rename(columns={col_i:'Value'})\n",
    "            df_i = df_i.dropna()\n",
    "            df_i['Event'] = col_i\n",
    "            if len(df_i) == 0: continue\n",
    "            elif df_final is None:\n",
    "                df_final = df_i\n",
    "            else:\n",
    "                df_final = pd.concat([df_final, df_i], ignore_index=True)\n",
    "        # print(df_final.head(2))\n",
    "        df_final['Value'] = df_final['Value'].astype(str)\n",
    "        if not df_final is None:\n",
    "            df_final.to_parquet(os.path.join(DATA_DIR, f'{split}/{fname}.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_parquet('train', TRAIN_FILES)\n",
    "csv_to_parquet('test', TEST_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files(keep_keywords = ['train', 'test', 'listfile']):\n",
    "    for files, split in [(TRAIN_FILES, 'train'), (TEST_FILES, 'test')]:\n",
    "        for filename in files:\n",
    "            if not (filename[-3:] == 'csv') or filename == 'listfile.csv': \n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(os.path.join(DATA_DIR, split), filename)\n",
    "            \n",
    "            if not any(keyword in filename.lower() for keyword in keep_keywords):\n",
    "                os.remove(file_path)\n",
    "                # print(f\"Deleted: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, 'train', 'listfile.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b4c24",
   "metadata": {},
   "source": [
    "COMPUTE STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a73cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats(xs):\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    x = np.arange(len(xs))\n",
    "    slope, _, _, _, _ = linregress(x, xs)\n",
    "\n",
    "    return pd.Series({\n",
    "        'min': xs.min(),\n",
    "        'max': xs.max(),\n",
    "        'mean': xs.mean(),\n",
    "        'std': xs.std(),\n",
    "        'median': np.median(xs),\n",
    "        'skew': skew(xs),\n",
    "        'kurtosis': kurtosis(xs),\n",
    "        'slope': slope,\n",
    "        'qr_25': np.quantile(xs, 0.25),\n",
    "        'qr_75': np.quantile(xs, 0.75),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "EYE = ['Spontaneously', 'To Speech', 'To Pain'] \n",
    "MOTOR = ['No response', 'Localizes Pain', 'Flex-withdraws', 'Abnormal Flexion', 'Obeys Commands', 'Abnormal extension']\n",
    "VERBAL = ['Incomprehensible sounds', 'No Response-ETT', 'Confused', 'Oriented', 'No Response', 'Inappropriate Words']\n",
    "\n",
    "def parse_gcs(x):\n",
    "    if x['Event'] == 'Glascow coma scale eye opening':\n",
    "        return EYE.index(x['Value'])\n",
    "    elif x['Event'] == 'Glascow coma scale motor response':\n",
    "        return MOTOR.index(x['Value'])\n",
    "    elif x['Event'] == 'Glascow coma scale verbal response':\n",
    "        return VERBAL.index(x['Value'])\n",
    "    else:\n",
    "        return x['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eccff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(split, files):\n",
    "    for file in files:\n",
    "        # print(file[-7:])\n",
    "        if not file[-7:] == 'parquet': \n",
    "            continue\n",
    "        df = pd.read_parquet(os.path.join(DATA_DIR, split, file))\n",
    "        if not 'Value' in df.columns: continue\n",
    "        df['Value'] = df.apply(parse_gcs, axis=1)\n",
    "        df['Value'] = pd.to_numeric(df['Value'])\n",
    "\n",
    "        df = (df.sort_values('Hours')[['Event', 'Value']]).groupby('Event').agg(list).reset_index()\n",
    "        df2 = df['Value'].apply(extract_stats).apply(pd.Series)\n",
    "\n",
    "        df = df.drop(columns=['Value'])\n",
    "        df = pd.concat([df, df2], axis=1)\n",
    "        df.to_parquet(os.path.join(DATA_DIR, split, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dedcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_stats('train', TRAIN_FILES)\n",
    "compute_stats('test', TEST_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56868b76",
   "metadata": {},
   "source": [
    "COMBINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(split, files):\n",
    "    df_combine = None\n",
    "    for file in files:\n",
    "        pat_id, episode, _ = file.split('_')\n",
    "        visit_num = episode[7:]\n",
    "        df = pd.read_parquet(os.path.join(DATA_DIR, split, file))\n",
    "        df['patient_id'] = int(pat_id)\n",
    "        df['visit_num'] = int(visit_num)\n",
    "\n",
    "        if df_combine is None:\n",
    "            df_combine = df\n",
    "        else:\n",
    "            df_combine = pd.concat([df, df_combine], ignore_index=True)\n",
    "    return df_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = combine_files('train', TRAIN_FILES)\n",
    "train_df.to_parquet(os.path.join(DATA_DIR, 'train.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = combine_files('test', TEST_FILES)\n",
    "test_df.to_parquet(os.path.join(DATA_DIR, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files(keep_keywords = ['train', 'test', 'listfile']):\n",
    "    for files, split in [(TRAIN_FILES, 'train'), (TEST_FILES, 'test')]:\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(os.path.join(DATA_DIR, split), filename)\n",
    "            \n",
    "            if not any(keyword in filename.lower() for keyword in keep_keywords):\n",
    "                os.remove(file_path)\n",
    "                # print(f\"Deleted: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddf9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f0b8a",
   "metadata": {},
   "source": [
    "PIVOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63652d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_df(df):\n",
    "    # df['visit_num'] = df.groupby(['patient_id', 'Event']).cumcount() + 1\n",
    "    # df['pivot_col'] = df['Event'] + '_' + df['visit_num'].astype(str)\n",
    "    # df = df[df['visit_num'] == 1]\n",
    "\n",
    "    df_pivoted = df.pivot(index=['patient_id', 'visit_num'], columns='Event')\n",
    "    df_pivoted.columns = [f'{col}_{event}' for col, event in df_pivoted.columns.swaplevel()]\n",
    "    df_pivoted = df_pivoted.reindex(sorted(df_pivoted.columns), axis=1)\n",
    "    df_pivoted = df_pivoted.reset_index()\n",
    "\n",
    "    df_pivoted['patient_id'] = pd.to_numeric(df_pivoted['patient_id'])\n",
    "    return df_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534282b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(os.path.join(DATA_DIR, 'train.parquet'))\n",
    "train_df = pivot_df(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_parquet(os.path.join(DATA_DIR, 'train.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet(os.path.join(DATA_DIR, 'test.parquet'))\n",
    "test_df = pivot_df(test_df)\n",
    "test_df.to_parquet(os.path.join(DATA_DIR, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a0eac",
   "metadata": {},
   "source": [
    "DEMOGRAPHICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13baccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca57230",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = os.listdir(os.path.join(DATA_DIR, 'test'))\n",
    "train_paths = os.listdir(os.path.join(DATA_DIR, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "RACES = []\n",
    "GENDER = []\n",
    "ETHNICITY = []\n",
    "\n",
    "for path in test_paths:\n",
    "    df1 = pd.read_csv(os.path.join(DATA_DIR, 'test', path, 'stays.csv'), usecols=['AGE', 'gender', 'race'])\n",
    "    df2 = pd.read_csv(os.path.join(DATA_DIR, 'test', path, 'episode1.csv'), usecols=['Ethnicity'])\n",
    "    df1 = df1.drop_duplicates()\n",
    "    df2 = df2.drop_duplicates()\n",
    "    df1 = df1[~((df1['race']=='UNKNOWN') | (df1['race']=='OTHER') \n",
    "                | (df1['race']=='UNABLE TO OBTAIN') | (df1['race']=='PATIENT DECLINED TO ANSWER'))]\n",
    "    RACES += list(df1['race'].unique())\n",
    "    GENDER += list(df1['gender'].unique())\n",
    "    ETHNICITY += list(df2['Ethnicity'].unique())\n",
    "RACES = list(set(RACES))\n",
    "GENDER = list(set(GENDER))\n",
    "ETHNICITY = list(set(ETHNICITY))\n",
    "\n",
    "RACES.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c1bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demo(split, file_paths):\n",
    "    demo_info = {}\n",
    "\n",
    "    for path in file_paths:\n",
    "        race, gender, age, ethnicity = None, None, None, None\n",
    "\n",
    "        df1 = pd.read_csv(os.path.join(DATA_DIR, split, path, 'stays.csv'), usecols=['AGE', 'gender', 'race'])\n",
    "        df2 = pd.read_csv(os.path.join(DATA_DIR, split, path, 'episode1.csv'), usecols=['Ethnicity'])\n",
    "        df1 = df1.drop_duplicates()\n",
    "        df2 = df2.drop_duplicates()\n",
    "        \n",
    "        df1 = df1[~((df1['race']=='UNKNOWN') | (df1['race']=='OTHER') \n",
    "                    | (df1['race']=='UNABLE TO OBTAIN') | (df1['race']=='PATIENT DECLINED TO ANSWER'))]\n",
    "        \n",
    "        if len(df1) >= 1:\n",
    "            race = max(list(df1['race'].unique()), key=len)\n",
    "            gender = max(list(df1['gender'].unique()), key=len)\n",
    "            age = max(list(df1['AGE'].unique()))\n",
    "\n",
    "        if len(df2) >= 1:\n",
    "            ethnicity = max(list(df2['Ethnicity'].unique()))\n",
    "        \n",
    "        demo_info[path] = (race, ethnicity, gender, age)\n",
    "    \n",
    "    demo_df = pd.DataFrame(demo_info).T\n",
    "    demo_df.columns = ['race', 'ethnicity', 'gender', 'age']\n",
    "\n",
    "    demo_df['race_ind'] = demo_df['race'].apply(lambda x: RACES.index(x) if x in RACES else x)\n",
    "    demo_df['gender_ind'] = demo_df['gender'].apply(lambda x: GENDER.index(x) if x in GENDER else x)\n",
    "\n",
    "    demo_df['gender'] = demo_df['gender'].fillna('unknown')\n",
    "    demo_df['race'] = demo_df['race'].fillna('unknown')\n",
    "    demo_df['race_ind'] = demo_df['race_ind'].fillna(29)\n",
    "    demo_df['gender_ind'] = demo_df['gender_ind'].fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = os.listdir(os.path.join(DATA_DIR, 'train'))\n",
    "train_demo = get_demo('train', train_paths)\n",
    "train_demo.to_parquet('data/root/train_demo.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = os.listdir(os.path.join(DATA_DIR, 'test'))\n",
    "test_demo = get_demo('test', test_paths)\n",
    "test_demo.to_parquet('data/root/test_demo.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d037a0",
   "metadata": {},
   "source": [
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4266ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(label_df):\n",
    "    label_df = label_df.rename(columns={'y_true':'Label'})\n",
    "    label_df['patient_id'] = pd.to_numeric(label_df['stay'].apply(lambda x: x.split('_')[0]))\n",
    "    label_df['visit_num'] = pd.to_numeric(label_df['stay'].apply(lambda x: x.split('_')[1][7:]))\n",
    "    label_df = label_df[['patient_id', 'visit_num', 'Label']] # period_length\n",
    "    return label_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eeacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_labels.csv'))\n",
    "train_df = process_label(train_df)\n",
    "train_df.to_csv(os.path.join(DATA_DIR, 'train_labels.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1104f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_labels.csv'))\n",
    "test_df = process_label(test_df)\n",
    "test_df.to_csv(os.path.join(DATA_DIR, 'test_labels.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc8e028",
   "metadata": {},
   "source": [
    "GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40112a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "EYE, MOTOR, VERBAL = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c977e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcs_values(split, files):\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(os.path.join(DATA_DIR, split, file))\n",
    "        EYE += list(df[df['Event'] == 'Glascow coma scale eye opening']['Value'].unique())\n",
    "        EYE = list(set(EYE))\n",
    "\n",
    "        MOTOR += list(df[df['Event'] == 'Glascow coma scale motor response']['Value'].unique())\n",
    "        MOTOR = list(set(MOTOR))\n",
    "\n",
    "        VERBAL += list(df[df['Event'] == 'Glascow coma scale verbal response']['Value'].unique())\n",
    "        VERBAL = list(set(VERBAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = os.listdir(os.path.join(DATA_DIR, 'train'))\n",
    "get_gcs_values('train', train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir(os.path.join(DATA_DIR, 'test'))\n",
    "get_gcs_values('test', test_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ised",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
